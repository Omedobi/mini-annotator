{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0438a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install labelImg\n",
    "import json\n",
    "import shutil\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import random \n",
    "import ipywidgets\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('Display.max_rows', None)\n",
    "pd.set_option('Display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.159  Python-3.9.18 torch-2.7.1+cpu CPU (Intel Core(TM) i7-8750H 2.20GHz)\n",
      "Setup complete  (12 CPUs, 23.9 GB RAM, 236.8/476.2 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b4cbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data.yaml', 'exports', 'test', 'train', 'val', 'yolo', '__init__.py']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = \"C:/Users/Legion/OneDrive/Documents/ML-Ops/mini-annotator/notebook/dataset/\"\n",
    "\n",
    "#check for subfolders present\n",
    "os.listdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.241 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.159  Python-3.9.18 torch-2.7.1+cpu CPU (Intel Core(TM) i7-8750H 2.20GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=64, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:/Users/Legion/OneDrive/Documents/ML-Ops/mini-annotator/notebook/dataset/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8m-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train24, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\segment\\train24, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=23\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   5172341  ultralytics.nn.modules.head.Segment          [23, 32, 192, [192, 384, 576]]\n",
      "YOLOv8m-seg summary: 191 layers, 27,252,965 parameters, 27,252,949 gradients, 110.5 GFLOPs\n",
      "\n",
      "Transferred 531/537 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 39.916.1 MB/s, size: 12.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Legion\\OneDrive\\Documents\\ML-Ops\\mini-annotator\\notebook\\dataset\\train\\labels.cache... 20 images, 0 backgrounds, 0 corrupt: 100%|██████████| 20/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 96.121.5 MB/s, size: 12.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Legion\\OneDrive\\Documents\\ML-Ops\\mini-annotator\\notebook\\dataset\\val\\labels.cache... 7 images, 0 backgrounds, 0 corrupt: 100%|██████████| 7/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\segment\\train24\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00037, momentum=0.9) with parameter groups 86 weight(decay=0.0), 97 weight(decay=0.0005), 96 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\segment\\train24\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20         0G      2.357      8.042      5.358      1.898        175        640: 100%|██████████| 1/1 [00:55<00:00, 55.69s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:07<00:00,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44    0.00411      0.115     0.0249    0.00444    0.00167     0.0615     0.0184    0.00557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20         0G      2.691      7.456       5.54      2.093        158        640: 100%|██████████| 1/1 [00:52<00:00, 52.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:08<00:00,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44     0.0065      0.103     0.0236    0.00468    0.00178     0.0615     0.0161    0.00501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20         0G      2.517      7.751       5.43      2.124        129        640: 100%|██████████| 1/1 [00:56<00:00, 56.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44    0.00672      0.103     0.0229    0.00517    0.00214     0.0615     0.0143    0.00507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20         0G      2.731      6.923      5.321      2.125        205        640: 100%|██████████| 1/1 [00:49<00:00, 49.94s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44    0.00995      0.154     0.0263    0.00542    0.00283     0.0744     0.0147    0.00471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20         0G      2.276      7.143      5.017      1.865        193        640: 100%|██████████| 1/1 [00:51<00:00, 51.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44     0.0127      0.179     0.0282    0.00545    0.00337     0.0872     0.0134    0.00404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20         0G      2.347      6.543      4.762      1.875        193        640: 100%|██████████| 1/1 [1:45:03<00:00, 6303.56s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:22<00:00, 22.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44     0.0153      0.218     0.0296       0.01     0.0064      0.113     0.0166     0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20         0G      2.381      5.263      4.776      1.797        203        640: 100%|██████████| 1/1 [02:49<00:00, 169.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:14<00:00, 14.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44     0.0158      0.218     0.0458     0.0226     0.0069      0.113     0.0162    0.00559\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20         0G      2.295       4.58      4.522      1.863        178        640: 100%|██████████| 1/1 [01:52<00:00, 112.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:07<00:00,  7.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44     0.0217       0.22      0.049     0.0268     0.0136       0.13     0.0188    0.00673\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20         0G      2.293      4.432      4.218      1.878        173        640: 100%|██████████| 1/1 [01:08<00:00, 68.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:07<00:00,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44     0.0215      0.228      0.051     0.0279     0.0129      0.138     0.0203    0.00788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20         0G      2.056      3.983      4.112      1.723        187        640: 100%|██████████| 1/1 [01:17<00:00, 77.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:07<00:00,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44     0.0319      0.318     0.0594     0.0291     0.0134      0.203     0.0202    0.00773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20         0G      2.107      3.731      4.336      1.692        111        640: 100%|██████████| 1/1 [10:42:23<00:00, 38543.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:07<00:00,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44      0.736      0.103     0.0662     0.0316      0.716     0.0513      0.025     0.0098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20         0G      2.071      3.299      4.192      1.772        101        640: 100%|██████████| 1/1 [00:52<00:00, 52.98s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:07<00:00,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44      0.896      0.103     0.0774     0.0349      0.874     0.0513     0.0306     0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20         0G      2.212      3.869      4.207      1.896         96        640: 100%|██████████| 1/1 [00:49<00:00, 49.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44        0.9      0.101     0.0955     0.0392       0.88     0.0513     0.0339     0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20         0G       2.25      2.821      3.821      1.808        107        640: 100%|██████████| 1/1 [00:46<00:00, 46.89s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44      0.523      0.141      0.101     0.0406      0.883     0.0513     0.0406     0.0155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20         0G      2.207      2.765      3.818      1.743        109        640: 100%|██████████| 1/1 [00:46<00:00, 46.92s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44      0.757      0.141      0.113     0.0464      0.884     0.0513     0.0472     0.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20         0G      1.881      2.428      3.535      1.613        106        640: 100%|██████████| 1/1 [00:48<00:00, 48.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44      0.844      0.137      0.115     0.0478      0.808     0.0705       0.05       0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20         0G      1.912      2.376      3.006      1.728         96        640: 100%|██████████| 1/1 [00:47<00:00, 47.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44      0.858      0.113      0.131     0.0635      0.842     0.0878     0.0623     0.0226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20         0G      1.839      2.485      2.965      1.611        105        640: 100%|██████████| 1/1 [00:45<00:00, 45.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44      0.846      0.111      0.142     0.0686      0.831     0.0864     0.0693     0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20         0G      1.854      2.233      3.162      1.536        106        640: 100%|██████████| 1/1 [00:46<00:00, 46.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44       0.84      0.113      0.157     0.0742      0.825     0.0888      0.085     0.0313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20         0G      1.756      2.528      2.951      1.553         99        640: 100%|██████████| 1/1 [00:46<00:00, 46.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44      0.868     0.0812      0.164       0.08      0.823     0.0896     0.0884     0.0338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 12.828 hours.\n",
      "Optimizer stripped from runs\\segment\\train24\\weights\\last.pt, 54.8MB\n",
      "Optimizer stripped from runs\\segment\\train24\\weights\\best.pt, 54.8MB\n",
      "\n",
      "Validating runs\\segment\\train24\\weights\\best.pt...\n",
      "Ultralytics 8.3.159  Python-3.9.18 torch-2.7.1+cpu CPU (Intel Core(TM) i7-8750H 2.20GHz)\n",
      "YOLOv8m-seg summary (fused): 105 layers, 27,235,701 parameters, 0 gradients, 110.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:05<00:00,  5.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44      0.839      0.114       0.16     0.0775      0.823     0.0897     0.0842     0.0315\n",
      "              Dropdown          3          3          1          0          0          0          1          0          0          0\n",
      "              Face cap          1          1          1          0          0          0          1          0          0          0\n",
      "        Hamburger icon          2          2          1          0      0.188     0.0941          1          0      0.188     0.0753\n",
      "           Input field          1          6          1          0      0.165     0.0699          1          0      0.165     0.0789\n",
      "                Jacket          2          5          1          0      0.104     0.0213          1          0      0.073     0.0252\n",
      "                   Man          2          3      0.393      0.651      0.591      0.472        0.2      0.333     0.0755     0.0302\n",
      "           Search Icon          2          2          1          0      0.137     0.0824          1          0      0.137     0.0586\n",
      "                Slider          1          1          1          0          0          0          1          0          0          0\n",
      "                 Woman          1          1          1          0          0          0          1          0          0          0\n",
      "                button          4          9      0.276      0.333      0.149      0.112      0.265      0.333      0.149     0.0971\n",
      "           cancel icon          2          2          1          0      0.497     0.0777          1          0          0          0\n",
      "                  dope          2          5          1          0          0          0          1          0          0          0\n",
      "             love icon          3          4      0.235        0.5       0.25     0.0774      0.234        0.5      0.307     0.0445\n",
      "Speed: 4.9ms preprocess, 706.6ms inference, 0.0ms loss, 103.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\segment\\train24\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#loading the model\n",
    "model = YOLO('yolov8m-seg.pt')\n",
    "\n",
    "#train the model\n",
    "results = model.train(data=os.path.join(root_dir,\"data.yaml\"), device='cpu', epochs=20, batch=64, name='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize Training Results\n",
    "# result_plot = \"C:/Users/Legion/OneDrive/Documents/ML-Ops/mini-annotator/notebook/runs/detect/train\"\n",
    "# if os.path.exists(result_plot):\n",
    "#     image = plt.imread(result_plot)\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "# else:\n",
    "#     print(f\"Training results not found at {result_plot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c6ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.159  Python-3.9.18 torch-2.7.1+cpu CPU (Intel Core(TM) i7-8750H 2.20GHz)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 52.327.9 MB/s, size: 12.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Legion\\OneDrive\\Documents\\ML-Ops\\mini-annotator\\notebook\\dataset\\val\\labels.cache... 7 images, 0 backgrounds, 0 corrupt: 100%|██████████| 7/7 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:05<00:00,  5.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          7         44      0.839      0.114       0.16     0.0775      0.823     0.0897     0.0842     0.0315\n",
      "              Dropdown          3          3          1          0          0          0          1          0          0          0\n",
      "              Face cap          1          1          1          0          0          0          1          0          0          0\n",
      "        Hamburger icon          2          2          1          0      0.188     0.0941          1          0      0.188     0.0753\n",
      "           Input field          1          6          1          0      0.165     0.0699          1          0      0.165     0.0789\n",
      "                Jacket          2          5          1          0      0.104     0.0213          1          0      0.073     0.0252\n",
      "                   Man          2          3      0.393      0.651      0.591      0.472        0.2      0.333     0.0755     0.0302\n",
      "           Search Icon          2          2          1          0      0.137     0.0824          1          0      0.137     0.0586\n",
      "                Slider          1          1          1          0          0          0          1          0          0          0\n",
      "                 Woman          1          1          1          0          0          0          1          0          0          0\n",
      "                button          4          9      0.276      0.333      0.149      0.112      0.265      0.333      0.149     0.0971\n",
      "           cancel icon          2          2          1          0      0.497     0.0777          1          0          0          0\n",
      "                  dope          2          5          1          0          0          0          1          0          0          0\n",
      "             love icon          3          4      0.235        0.5       0.25     0.0774      0.234        0.5      0.307     0.0445\n",
      "Speed: 2.9ms preprocess, 651.0ms inference, 0.0ms loss, 93.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\segment\\valid2\u001b[0m\n",
      "Mean Average Precision (mAP): 0.07745448787143526\n",
      "Precision: [          1           1           1           1           1     0.39294           1           1           1     0.27638           1           1     0.23536]\n",
      "Recall: [          0           0           0           0           0     0.65098           0           0           0     0.33333           0           0         0.5]\n"
     ]
    }
   ],
   "source": [
    "#Model evaluation\n",
    "metrics = model.val(data=\"C:/Users/Legion/OneDrive/Documents/ML-Ops/mini-annotator/notebook/dataset/data.yaml\", name='valid')\n",
    "\n",
    "# Access specific metrics\n",
    "print(\"Mean Average Precision (mAP):\", metrics.box.map)  \n",
    "print(\"Precision:\", metrics.box.p)\n",
    "print(\"Recall:\", metrics.box.r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on new images\n",
    "model = YOLO(r'C:\\Users\\Legion\\OneDrive\\Documents\\ML-Ops\\mini-annotator\\notebook\\runs\\segment\\train24\\weights\\best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40659cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in directory: ['Screenshot_20240830_180646.png', 'Screenshot_20240830_180710.png', 'Screenshot_20240830_180916.png', 'Screenshot_20240830_180928.png', 'ui-element1.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Define the test directory\n",
    "test_dir = r\"C:\\Users\\Legion\\OneDrive\\Documents\\ML-Ops\\UI.Land Computer vision\\ui-element-detection\\test\\images\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(test_dir):\n",
    "    raise FileNotFoundError(f\"Test directory not found: {test_dir}\")\n",
    "\n",
    "# List files in the directory\n",
    "files = os.listdir(test_dir)\n",
    "print(\"Files in directory:\", files)\n",
    "\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No files found in the directory: {test_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72d35d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/5 C:\\Users\\Legion\\OneDrive\\Documents\\ML-Ops\\UI.Land Computer vision\\ui-element-detection\\test\\images\\Screenshot_20240830_180646.png: 640x320 1 Man, 2 Search Icons, 3 T shirts, 3 love icons, 313.4ms\n",
      "image 2/5 C:\\Users\\Legion\\OneDrive\\Documents\\ML-Ops\\UI.Land Computer vision\\ui-element-detection\\test\\images\\Screenshot_20240830_180710.png: 640x320 3 Mans, 3 T shirts, 2 love icons, 316.3ms\n",
      "image 3/5 C:\\Users\\Legion\\OneDrive\\Documents\\ML-Ops\\UI.Land Computer vision\\ui-element-detection\\test\\images\\Screenshot_20240830_180916.png: 640x320 3 Mans, 2 T shirts, 1 love icon, 271.3ms\n",
      "image 4/5 C:\\Users\\Legion\\OneDrive\\Documents\\ML-Ops\\UI.Land Computer vision\\ui-element-detection\\test\\images\\Screenshot_20240830_180928.png: 640x320 3 Mans, 1 Search Icon, 1 love icon, 286.9ms\n",
      "image 5/5 C:\\Users\\Legion\\OneDrive\\Documents\\ML-Ops\\UI.Land Computer vision\\ui-element-detection\\test\\images\\ui-element1.jpg: 640x640 (no detections), 565.9ms\n",
      "Speed: 3.5ms preprocess, 350.8ms inference, 6.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\segment\\test_results\u001b[0m\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: ultralytics.engine.results.Masks object\n",
      "names: {0: 'Dropdown', 1: 'Exist', 2: 'Face cap', 3: 'Filter icon', 4: 'Hamburger icon', 5: 'Input field', 6: 'Jacket', 7: 'Loading Icon', 8: 'Man', 9: 'Search Icon', 10: 'Slider', 11: 'Sneakers', 12: 'T shirt', 13: 'Woman', 14: 'button', 15: 'cancel icon', 16: 'clothing icon', 17: 'dope', 18: 'dopeb', 19: 'love icon', 20: 'radio', 21: 'shareicon', 22: 'socks'}\n",
      "obb: None\n",
      "orig_img: array([[[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]]], dtype=uint8)\n",
      "orig_shape: (1507, 720)\n",
      "path: 'C:\\\\Users\\\\Legion\\\\OneDrive\\\\Documents\\\\ML-Ops\\\\UI.Land Computer vision\\\\ui-element-detection\\\\test\\\\images\\\\Screenshot_20240830_180646.png'\n",
      "probs: None\n",
      "save_dir: 'runs\\\\segment\\\\test_results'\n",
      "speed: {'preprocess': 3.430800003116019, 'inference': 313.44220000028145, 'postprocess': 11.554300006537233}\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: ultralytics.engine.results.Masks object\n",
      "names: {0: 'Dropdown', 1: 'Exist', 2: 'Face cap', 3: 'Filter icon', 4: 'Hamburger icon', 5: 'Input field', 6: 'Jacket', 7: 'Loading Icon', 8: 'Man', 9: 'Search Icon', 10: 'Slider', 11: 'Sneakers', 12: 'T shirt', 13: 'Woman', 14: 'button', 15: 'cancel icon', 16: 'clothing icon', 17: 'dope', 18: 'dopeb', 19: 'love icon', 20: 'radio', 21: 'shareicon', 22: 'socks'}\n",
      "obb: None\n",
      "orig_img: array([[[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]]], dtype=uint8)\n",
      "orig_shape: (1507, 720)\n",
      "path: 'C:\\\\Users\\\\Legion\\\\OneDrive\\\\Documents\\\\ML-Ops\\\\UI.Land Computer vision\\\\ui-element-detection\\\\test\\\\images\\\\Screenshot_20240830_180710.png'\n",
      "probs: None\n",
      "save_dir: 'runs\\\\segment\\\\test_results'\n",
      "speed: {'preprocess': 3.0989999941084534, 'inference': 316.3122999976622, 'postprocess': 7.803399996191729}\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: ultralytics.engine.results.Masks object\n",
      "names: {0: 'Dropdown', 1: 'Exist', 2: 'Face cap', 3: 'Filter icon', 4: 'Hamburger icon', 5: 'Input field', 6: 'Jacket', 7: 'Loading Icon', 8: 'Man', 9: 'Search Icon', 10: 'Slider', 11: 'Sneakers', 12: 'T shirt', 13: 'Woman', 14: 'button', 15: 'cancel icon', 16: 'clothing icon', 17: 'dope', 18: 'dopeb', 19: 'love icon', 20: 'radio', 21: 'shareicon', 22: 'socks'}\n",
      "obb: None\n",
      "orig_img: array([[[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [ 41,  43,  43],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [ 40,  42,  42],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [ 39,  41,  41],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]]], dtype=uint8)\n",
      "orig_shape: (1507, 720)\n",
      "path: 'C:\\\\Users\\\\Legion\\\\OneDrive\\\\Documents\\\\ML-Ops\\\\UI.Land Computer vision\\\\ui-element-detection\\\\test\\\\images\\\\Screenshot_20240830_180916.png'\n",
      "probs: None\n",
      "save_dir: 'runs\\\\segment\\\\test_results'\n",
      "speed: {'preprocess': 2.5374000033480115, 'inference': 271.3376999963657, 'postprocess': 5.616800001007505}\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: ultralytics.engine.results.Masks object\n",
      "names: {0: 'Dropdown', 1: 'Exist', 2: 'Face cap', 3: 'Filter icon', 4: 'Hamburger icon', 5: 'Input field', 6: 'Jacket', 7: 'Loading Icon', 8: 'Man', 9: 'Search Icon', 10: 'Slider', 11: 'Sneakers', 12: 'T shirt', 13: 'Woman', 14: 'button', 15: 'cancel icon', 16: 'clothing icon', 17: 'dope', 18: 'dopeb', 19: 'love icon', 20: 'radio', 21: 'shareicon', 22: 'socks'}\n",
      "obb: None\n",
      "orig_img: array([[[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]]], dtype=uint8)\n",
      "orig_shape: (1507, 720)\n",
      "path: 'C:\\\\Users\\\\Legion\\\\OneDrive\\\\Documents\\\\ML-Ops\\\\UI.Land Computer vision\\\\ui-element-detection\\\\test\\\\images\\\\Screenshot_20240830_180928.png'\n",
      "probs: None\n",
      "save_dir: 'runs\\\\segment\\\\test_results'\n",
      "speed: {'preprocess': 4.469500003324356, 'inference': 286.9444000025396, 'postprocess': 6.297100000665523}\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'Dropdown', 1: 'Exist', 2: 'Face cap', 3: 'Filter icon', 4: 'Hamburger icon', 5: 'Input field', 6: 'Jacket', 7: 'Loading Icon', 8: 'Man', 9: 'Search Icon', 10: 'Slider', 11: 'Sneakers', 12: 'T shirt', 13: 'Woman', 14: 'button', 15: 'cancel icon', 16: 'clothing icon', 17: 'dope', 18: 'dopeb', 19: 'love icon', 20: 'radio', 21: 'shareicon', 22: 'socks'}\n",
      "obb: None\n",
      "orig_img: array([[[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]]], dtype=uint8)\n",
      "orig_shape: (640, 640)\n",
      "path: 'C:\\\\Users\\\\Legion\\\\OneDrive\\\\Documents\\\\ML-Ops\\\\UI.Land Computer vision\\\\ui-element-detection\\\\test\\\\images\\\\ui-element1.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs\\\\segment\\\\test_results'\n",
      "speed: {'preprocess': 3.9242999991984107, 'inference': 565.8732000010787, 'postprocess': 0.4193999993731268}\n"
     ]
    }
   ],
   "source": [
    "# path to the test folder\n",
    "test_dir = r\"C:\\Users\\Legion\\OneDrive\\Documents\\ML-Ops\\UI.Land Computer vision\\ui-element-detection\\test\\images\"\n",
    "\n",
    "results = model.predict(source=test_dir, save=True, imgsz=640, conf=0.25, name='test_results')\n",
    "\n",
    "#show the result\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c9975a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No images (.jpg) found in ./notebook/runs/segment/test_results/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#The folder where results are saved\n",
    "results_dir = './notebook/runs/segment/test_results/' \n",
    "\n",
    "#Find all images in that directory\n",
    "image_files = list(Path(results_dir).glob(\"*.jpg\")) + list(Path(results_dir).glob(\"*.png\"))\n",
    "\n",
    "if image_files:\n",
    "    \n",
    "    predicted_image_path = str(image_files[0])\n",
    "    print(f\"Displaying: {predicted_image_path}\")\n",
    "    \n",
    "    image = cv2.imread(predicted_image_path)\n",
    "    if image is not None:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"RT-DETR Prediction\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Error: Could not read the image file.\")\n",
    "else:\n",
    "    print(f\"No images (.jpg) found in {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422233b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cb4309f",
   "metadata": {},
   "source": [
    "Using Real Time Detection Transformer RF-DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91aaf356",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inference'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Load the model\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'inference'"
     ]
    }
   ],
   "source": [
    "from inference import get_model\n",
    "import cv2\n",
    "\n",
    "#Load the model\n",
    "model = get_model(\"rf-detr/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Load your image (using PIL like in your previous scripts)\n",
    "image = Image.open(\"path/to/your/image.jpg\")\n",
    "\n",
    "#Predict\n",
    "detections = model.predict(image, threshold=0.5)\n",
    "\n",
    "# Visualize using Supervision\n",
    "annotator = sv.BoxAnnotator()\n",
    "annotated_image = annotator.annotate(scene=image.copy(), detections=detections)\n",
    "\n",
    "# Display or save\n",
    "annotated_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
